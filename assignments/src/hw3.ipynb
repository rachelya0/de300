{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aefe08a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, mean, create_map\n",
    "from pyspark.sql.types import FloatType\n",
    "from itertools import chain\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ccfab7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/ubuntu/anaconda3/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/ubuntu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/ubuntu/.ivy2/jars\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-70d350b8-2595-43fd-a865-5a0b6c4d83ec;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.375 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.4 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.12.262 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 330ms :: artifacts dl 11ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.12.262 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.4 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.375 by [com.amazonaws#aws-java-sdk-bundle;1.12.262] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   4   |   0   |   0   |   1   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-70d350b8-2595-43fd-a865-5a0b6c4d83ec\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/9ms)\n",
      "24/05/23 06:40:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/23 06:40:32 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=com.amazonaws:aws-java-sdk-bundle:1.11.375,org.apache.hadoop:hadoop-aws:3.3.4 pyspark-shell\"\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"HeartDiseasePrediction\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark._jsc.hadoopConfiguration().set(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"true\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "    \n",
    "# Set Hadoop configurations for S3 access\n",
    "hadoop_conf = spark._jsc.hadoopConfiguration()\n",
    "hadoop_conf.set(\"fs.s3.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "hadoop_conf.set(\"fs.s3a.access.key\", \"ASIAYAAO5HRMMSII5MG7\")\n",
    "hadoop_conf.set(\"fs.s3a.secret.key\", \"OXOxUhDW+4aAxjexr3QsgCMHy35J8wWxDqa/shv3\")\n",
    "hadoop_conf.set(\"fs.s3a.session.token\", \"IQoJb3JpZ2luX2VjEPb//////////wEaCXVzLWVhc3QtMiJHMEUCIQClVIaclPf1qpD10aFMKFFc4sQ7g0QVsMz63F+Fp3o9wAIgBojPY4RT1xV6MfzYhAxMGowLcpEn2xRuhsfW371brQgq6wIIbxAAGgw1NDk3ODcwOTAwMDgiDMQmT1U2Lq4YJILbFyrIAmYS9ZYmU2bHeq8n3rAVrxIFhPXSZxEUcsEmM7+N8Dp5EiHfQTqm27fcskKl24bN9p4Oc7q7eQH4rGHMCy1GGg2BLBm6S6LtDZDRK8Tgwm/RyFVKYK8mKBGwiu1WiIm4QGLLRKj1lKOP9bmi23ptb7Jb9yXA6dqMc+STQ3fiUWJwaxouQGxpwnhoiV3Quv6yVKPAPH7cYvInpBR6s8qF76mgkHPDFuUzbqFtj3q1+ZKvjQlwYM/+EyE5uhpFJfi+0ozFnfYn+YPrWgMdL3CV8pNaqLuTxh48fOj4uiTH8NFOPVK8CihGY5cAEnSdf//VGscwDyUPEdggrLOroyBVNNf29fiMrXOaTEXZWQ0fqPizM/LFaIHWsiPkPYLjl7NdwniM5pEnsYIp1Fq8Gy82NqgmZXYU+VOyUgxN2MuXmyXbNRjKGrNw078wmrq7sgY6pwGmimQF6BRLVCHfQ9irASgPPIs+ZfKh1Iz+D7MLLJiOUK6x4ftL3EZQzcVEN4p4MNk1X/u9aUx5tYKRVt1/XSpwvwGLf00laLNsJOtYOZ+6x2SAPCWoXmMQK8g52w5lcXkHKoM4nsrnigewC4vuNL/mjAQ7lyzc0mP3dCuVtsA5WETJ3+tBaYFsxAj4sjL49aLMmWXwrMKG+aP3iuAtJ3VlyngRaWqkHg==\")\n",
    "\n",
    "# Load data\n",
    "s3_bucket = \"s3a://de300spring2024\"\n",
    "file_key = 'rachel_yao/heart_disease(in).csv'\n",
    "data_path = f\"{s3_bucket}/{file_key}\"\n",
    "data = spark.read.csv(data_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3c6bfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select relevant columns and clean data\n",
    "retain = ['age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', 'smoke', \n",
    "          'fbs', 'prop', 'nitr', 'pro', 'diuretic', 'thaldur', 'thalach', \n",
    "          'exang', 'oldpeak', 'slope', 'target']\n",
    "data = data.select(retain)\n",
    "\n",
    "# Fill missing values and clean data\n",
    "data = data.fillna({\n",
    "    'painloc': data.groupBy().agg({\"painloc\": \"max\"}).collect()[0][0],\n",
    "    'painexer': data.groupBy().agg({\"painexer\": \"max\"}).collect()[0][0],\n",
    "    'trestbps': data.approxQuantile(\"trestbps\", [0.5], 0)[0],\n",
    "    'oldpeak': data.approxQuantile(\"oldpeak\", [0.5], 0)[0],\n",
    "    'thaldur': data.approxQuantile(\"thaldur\", [0.5], 0)[0],\n",
    "    'thalach': data.approxQuantile(\"thalach\", [0.5], 0)[0],\n",
    "    'sex': data.groupBy().agg({\"sex\": \"max\"}).collect()[0][0],\n",
    "    'cp': data.groupBy().agg({\"cp\": \"max\"}).collect()[0][0],\n",
    "})\n",
    "data = data.withColumn('trestbps', when(col('trestbps') < 100, lit(120)).otherwise(col('trestbps')))\n",
    "data = data.withColumn('oldpeak', when((col('oldpeak') < 0) | (col('oldpeak') > 4), lit(1)).otherwise(col('oldpeak')))\n",
    "data = data.withColumn('fbs', when(col('fbs') > 1, lit(0)).otherwise(col('fbs')))\n",
    "data = data.withColumn('prop', when(col('prop') > 1, lit(0)).otherwise(col('prop')))\n",
    "data = data.withColumn('nitr', when(col('nitr') > 1, lit(0)).otherwise(col('nitr')))\n",
    "data = data.withColumn('pro', when(col('pro') > 1, lit(0)).otherwise(col('pro')))\n",
    "data = data.withColumn('diuretic', when(col('diuretic') > 1, lit(0)).otherwise(col('diuretic')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3513fa31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape smoking rates by age from source 1\n",
    "def scrape_smoking_rates_by_age(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "    smoking_data = {}\n",
    "    for table in tables[1]:\n",
    "        rows = table.find_all('tr')\n",
    "        for row in rows[1:]:\n",
    "            ths = row.find_all('th')\n",
    "            tds = row.find_all('td')\n",
    "            age_range = ths[0].text.strip()\n",
    "            smoking_rate = float(tds[9].text.strip())\n",
    "            if 'and over' in age_range:\n",
    "                min_age = int(age_range.split()[0])\n",
    "                max_age = 120  # assuming 120 as an upper limit for age\n",
    "            else:\n",
    "                min_age, max_age = map(int, age_range.split('â€“'))\n",
    "            for age in range(min_age, max_age + 1):\n",
    "                smoking_data[age] = smoking_rate\n",
    "    return smoking_data\n",
    "\n",
    "# scrape smoking rates by age and sex from source 2\n",
    "def scrape_smoking_rates_by_age_and_sex(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    # sex\n",
    "    cards = soup.find_all(\"div\", class_=\"card-body\")\n",
    "    gender_data = {}\n",
    "    for card in cards[2:3]:\n",
    "        rows = card.find_all('li', class_='main')\n",
    "        for row in rows:\n",
    "            text = row.text.strip()\n",
    "            gender = text.split()[6]\n",
    "            rate = float(text.split()[7].strip('()%'))\n",
    "            gender_data[gender] = rate\n",
    "    # age\n",
    "    age_data = {}\n",
    "    for card in cards[3:4]:\n",
    "        rows = card.find_all('li')\n",
    "        for row in rows:\n",
    "            text = row.text.strip()\n",
    "            age_range = text.split()[7]\n",
    "            if 'and older' in text:\n",
    "                min_age = int(age_range.split()[0])\n",
    "                max_age = 120  # assuming 120 as an upper limit for age\n",
    "            else:\n",
    "                min_age, max_age = map(int, age_range.split('â€“'))\n",
    "                rate = float(text.split()[9].strip('()%'))\n",
    "            for age in range(min_age, max_age + 1):\n",
    "                age_data[age] = rate\n",
    "    return gender_data, age_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6beb497d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# source 1\n",
    "source1 = 'https://www.abs.gov.au/statistics/health/health-conditions-and-risks/smoking/latest-release'\n",
    "smoking_source1 = scrape_smoking_rates_by_age(source1)\n",
    "\n",
    "# source 2\n",
    "source2 = 'https://www.cdc.gov/tobacco/data_statistics/fact_sheets/adult_data/cig_smoking/index.htm'\n",
    "gender_data, age_data = scrape_smoking_rates_by_age_and_sex(source2)\n",
    "\n",
    "broadcast_smoking_source1 = spark.sparkContext.broadcast(smoking_source1)\n",
    "broadcast_gender_data = spark.sparkContext.broadcast(gender_data)\n",
    "broadcast_age_data = spark.sparkContext.broadcast(age_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2938db4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+--------+---+--------+---+----+----+---+--------+-------+-------+-----+-------+-----+------+-------------+------------------+\n",
      "|age|sex|painloc|painexer| cp|trestbps|fbs|prop|nitr|pro|diuretic|thaldur|thalach|exang|oldpeak|slope|target|smoke_source1|     smoke_source2|\n",
      "+---+---+-------+--------+---+--------+---+----+----+---+--------+-------+-------+-----+-------+-----+------+-------------+------------------+\n",
      "| 63|  1|      1|       1|  1|     145|  1|   0|   0|  0|       0|   10.5|    150|    0|    2.3|    3|     0|         14.9|19.325742574257426|\n",
      "| 67|  1|      1|       1|  4|     160|  0|   1|   0|  0|       0|    9.5|    108|    1|    1.5|    2|     1|          8.7|19.325742574257426|\n",
      "| 67|  1|      1|       1|  4|     120|  0|   1|   0|  0|       0|    8.5|    129|    1|    2.6|    2|     1|          8.7|19.325742574257426|\n",
      "| 37|  1|      1|       1|  3|     130|  0|   1|   0|  0|       0|   13.0|    187|    0|    3.5|    3|     0|         10.9|16.342574257425742|\n",
      "| 41|  0|      1|       1|  2|     130|  0|   0|   0|  0|       0|    7.0|    172|    0|    1.4|    1|     0|         10.9|              12.6|\n",
      "| 56|  1|      1|       1|  2|     120|  0|   0|   0|  0|       0|   11.3|    178|    0|    0.8|    1|     0|         14.9|19.325742574257426|\n",
      "| 62|  0|      1|       1|  4|     140|  0|   0|   0|  0|       0|    6.0|    160|    0|    3.6|    3|     1|         14.9|              14.9|\n",
      "| 57|  0|      1|       1|  4|     120|  0|   0|   0|  0|       0|    9.0|    163|    1|    0.6|    1|     0|         14.9|              14.9|\n",
      "| 63|  1|      1|       1|  4|     130|  0|   1|   1|  0|       0|    8.0|    147|    0|    1.4|    2|     1|         14.9|19.325742574257426|\n",
      "| 53|  1|      1|       1|  4|     140|  1|   1|   0|  0|       1|    5.5|    155|    1|    3.1|    3|     1|         13.8|19.325742574257426|\n",
      "| 57|  1|      1|       1|  4|     140|  0|   0|   0|  0|       0|    8.2|    148|    0|    0.4|    2|     0|         14.9|19.325742574257426|\n",
      "| 56|  0|      1|       1|  2|     140|  0|   1|   1|  0|       0|    4.5|    153|    0|    1.3|    2|     0|         14.9|              14.9|\n",
      "| 56|  1|      1|       1|  3|     130|  1|   0|   0|  0|       0|   13.0|    142|    1|    0.6|    2|     1|         14.9|19.325742574257426|\n",
      "| 44|  1|      1|       1|  2|     120|  0|   1|   0|  0|       0|    9.3|    173|    0|    0.0|    1|     0|         10.9|16.342574257425742|\n",
      "| 52|  1|      1|       1|  3|     172|  1|   0|   0|  0|       0|   12.5|    162|    0|    0.5|    1|     0|         13.8|19.325742574257426|\n",
      "| 57|  1|      1|       1|  3|     150|  0|   0|   1|  0|       0|   11.0|    174|    0|    1.6|    1|     0|         14.9|19.325742574257426|\n",
      "| 48|  1|      1|       1|  2|     110|  0|   1|   0|  0|       0|    9.8|    168|    0|    1.0|    3|     1|         13.8|19.325742574257426|\n",
      "| 54|  1|      1|       1|  4|     140|  0|   0|   0|  0|       1|    7.8|    160|    0|    1.2|    1|     0|         13.8|19.325742574257426|\n",
      "| 48|  0|      1|       1|  3|     130|  0|   0|   0|  0|       0|   10.0|    139|    0|    0.2|    1|     0|         13.8|              14.9|\n",
      "| 49|  1|      1|       1|  2|     130|  0|   0|   0|  0|       0|   12.0|    171|    0|    0.6|    1|     0|         13.8|19.325742574257426|\n",
      "+---+---+-------+--------+---+--------+---+----+----+---+--------+-------+-------+-----+-------+-----+------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create mappings for smoke imputation\n",
    "smoking_map_source1 = create_map([lit(x) for x in chain(*broadcast_smoking_source1.value.items())])\n",
    "smoking_map_age = create_map([lit(x) for x in chain(*broadcast_age_data.value.items())])\n",
    "men_rate = broadcast_gender_data.value['men']\n",
    "women_rate = broadcast_gender_data.value['women']\n",
    "\n",
    "# Impute smoke_source1\n",
    "data = data.withColumn('smoke_source1', \n",
    "                       when(col('smoke').isNull(), smoking_map_source1[col('age')])\n",
    "                       .otherwise(col('smoke')))\n",
    "\n",
    "# Impute smoke_source2\n",
    "data = data.withColumn('smoke_source2', \n",
    "                       when(col('smoke').isNull() & (col('sex') == 0), smoking_map_age[col('age')])\n",
    "                       .when(col('smoke').isNull() & (col('sex') == 1), smoking_map_age[col('age')] * (men_rate / women_rate))\n",
    "                       .otherwise(col('smoke')))\n",
    "\n",
    "# Drop original smoke column\n",
    "data = data.drop('smoke')\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "# Show data\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61c348be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 06:40:50 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|  282|\n",
      "|  1.0|  184|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|  0.0|   25|\n",
      "|  1.0|   15|\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/23 06:40:58 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Metrics:\n",
      "Accuracy: 0.775\n",
      "Precision: 0.7897727272727273\n",
      "Recall: 0.7749999999999999\n",
      "F1 Score: 0.778046421663443\n",
      "Random Forest Metrics:\n",
      "Accuracy: 0.85\n",
      "Precision: 0.8497150997150997\n",
      "Recall: 0.85\n",
      "F1 Score: 0.8475274725274724\n",
      "Gradient Boosting Metrics:\n",
      "Accuracy: 0.775\n",
      "Precision: 0.80625\n",
      "Recall: 0.775\n",
      "F1 Score: 0.7785714285714286\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "# Feature engineering\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Ensure 'target' column is categorical\n",
    "indexer = StringIndexer(inputCol=\"target\", outputCol=\"label\")\n",
    "data = indexer.fit(data).transform(data)\n",
    "\n",
    "# Define the feature columns\n",
    "feature_cols = ['age', 'sex', 'painloc', 'painexer', 'cp', 'trestbps', 'fbs', 'prop', 'nitr', 'pro', 'diuretic', 'thaldur', 'thalach', 'exang', 'oldpeak', 'slope', 'smoke_source1', 'smoke_source2']\n",
    "\n",
    "for col_name in feature_cols:\n",
    "    data = data.withColumn(col_name, col(col_name).cast('double'))\n",
    "\n",
    "# Assemble feature columns into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "data = assembler.transform(data)\n",
    "\n",
    "# Split the data into training and test sets with 90-10 split and stratification\n",
    "train_data, test_data = data.randomSplit([0.9, 0.1], seed=42)\n",
    "\n",
    "# Verify the splits\n",
    "train_data.groupBy(\"label\").count().show()\n",
    "test_data.groupBy(\"label\").count().show()\n",
    "\n",
    "# Initialize models\n",
    "lr = LogisticRegression(featuresCol='features', labelCol='label')\n",
    "rf = RandomForestClassifier(featuresCol='features', labelCol='label')\n",
    "gbt = GBTClassifier(featuresCol='features', labelCol='label')\n",
    "\n",
    "# Set up cross-validation\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "\n",
    "# Logistic Regression\n",
    "lr_param_grid = ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01]).build()\n",
    "lr_cv = CrossValidator(estimator=lr, estimatorParamMaps=lr_param_grid, evaluator=evaluator, numFolds=5)\n",
    "lr_model = lr_cv.fit(train_data)\n",
    "lr_accuracy = evaluator.evaluate(lr_model.transform(test_data))\n",
    "\n",
    "# Random Forest\n",
    "rf_param_grid = ParamGridBuilder().addGrid(rf.numTrees, [10, 20]).build()\n",
    "rf_cv = CrossValidator(estimator=rf, estimatorParamMaps=rf_param_grid, evaluator=evaluator, numFolds=5)\n",
    "rf_model = rf_cv.fit(train_data)\n",
    "rf_accuracy = evaluator.evaluate(rf_model.transform(test_data))\n",
    "\n",
    "# Gradient Boosting\n",
    "gbt_param_grid = ParamGridBuilder().addGrid(gbt.maxDepth, [5, 10]).build()\n",
    "gbt_cv = CrossValidator(estimator=gbt, estimatorParamMaps=gbt_param_grid, evaluator=evaluator, numFolds=5)\n",
    "gbt_model = gbt_cv.fit(train_data)\n",
    "gbt_accuracy = evaluator.evaluate(gbt_model.transform(test_data))\n",
    "\n",
    "def evaluate_model(predictions, label_col=\"label\"):\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol=label_col)\n",
    "    accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "    precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "    recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "    f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "    print(f\"Accuracy: {accuracy}\")\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "\n",
    "# Evaluate models after balancing\n",
    "lr_predictions = lr_model.transform(test_data)\n",
    "print(\"Logistic Regression Metrics:\")\n",
    "evaluate_model(lr_predictions)\n",
    "\n",
    "rf_predictions = rf_model.transform(test_data)\n",
    "print(\"Random Forest Metrics:\")\n",
    "evaluate_model(rf_predictions)\n",
    "\n",
    "gbt_predictions = gbt_model.transform(test_data)\n",
    "print(\"Gradient Boosting Metrics:\")\n",
    "evaluate_model(gbt_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c470468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random forest seems to perform the best out of the models tested, with a high mean accuracy of 0.85, precision of 0.85, recall of 0.85, and f1 score of 0.85.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578fe061",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
